{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legal-cooperation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "reported-portugal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1746213754449849"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-(math.log((math.exp(0.705917)-1))-0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sorted-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "superior-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 3]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([1,2,3,4,5,6,7],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-coalition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-kitchen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loved-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facial-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blessed-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (gcn1): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (gcn2): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 16, F.relu)\n",
    "        self.gcn2 = GCN(16, 7, None)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parental-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g = dgl.from_networkx(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chicken-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型评估\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hourly-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 00000 | Loss 1.1213 | Test Acc 0.6960 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.1074 | Test Acc 0.7010 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.0937 | Test Acc 0.7020 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.0803 | Test Acc 0.7010 | Time(s) 0.0142\n",
      "Epoch 00004 | Loss 1.0670 | Test Acc 0.7050 | Time(s) 0.0139\n",
      "Epoch 00005 | Loss 1.0538 | Test Acc 0.7080 | Time(s) 0.0133\n",
      "Epoch 00006 | Loss 1.0408 | Test Acc 0.7090 | Time(s) 0.0131\n",
      "Epoch 00007 | Loss 1.0280 | Test Acc 0.7110 | Time(s) 0.0130\n",
      "Epoch 00008 | Loss 1.0152 | Test Acc 0.7110 | Time(s) 0.0129\n",
      "Epoch 00009 | Loss 1.0026 | Test Acc 0.7100 | Time(s) 0.0135\n",
      "Epoch 00010 | Loss 0.9902 | Test Acc 0.7130 | Time(s) 0.0137\n",
      "Epoch 00011 | Loss 0.9779 | Test Acc 0.7150 | Time(s) 0.0136\n",
      "Epoch 00012 | Loss 0.9657 | Test Acc 0.7170 | Time(s) 0.0139\n",
      "Epoch 00013 | Loss 0.9537 | Test Acc 0.7160 | Time(s) 0.0139\n",
      "Epoch 00014 | Loss 0.9418 | Test Acc 0.7190 | Time(s) 0.0138\n",
      "Epoch 00015 | Loss 0.9300 | Test Acc 0.7170 | Time(s) 0.0140\n",
      "Epoch 00016 | Loss 0.9184 | Test Acc 0.7170 | Time(s) 0.0141\n",
      "Epoch 00017 | Loss 0.9069 | Test Acc 0.7180 | Time(s) 0.0140\n",
      "Epoch 00018 | Loss 0.8955 | Test Acc 0.7190 | Time(s) 0.0141\n",
      "Epoch 00019 | Loss 0.8842 | Test Acc 0.7220 | Time(s) 0.0141\n",
      "Epoch 00020 | Loss 0.8731 | Test Acc 0.7220 | Time(s) 0.0143\n",
      "Epoch 00021 | Loss 0.8621 | Test Acc 0.7260 | Time(s) 0.0143\n",
      "Epoch 00022 | Loss 0.8512 | Test Acc 0.7280 | Time(s) 0.0143\n",
      "Epoch 00023 | Loss 0.8405 | Test Acc 0.7280 | Time(s) 0.0144\n",
      "Epoch 00024 | Loss 0.8298 | Test Acc 0.7260 | Time(s) 0.0144\n",
      "Epoch 00025 | Loss 0.8193 | Test Acc 0.7270 | Time(s) 0.0143\n",
      "Epoch 00026 | Loss 0.8090 | Test Acc 0.7280 | Time(s) 0.0142\n",
      "Epoch 00027 | Loss 0.7987 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00028 | Loss 0.7886 | Test Acc 0.7280 | Time(s) 0.0141\n",
      "Epoch 00029 | Loss 0.7786 | Test Acc 0.7290 | Time(s) 0.0141\n",
      "Epoch 00030 | Loss 0.7687 | Test Acc 0.7300 | Time(s) 0.0142\n",
      "Epoch 00031 | Loss 0.7590 | Test Acc 0.7290 | Time(s) 0.0142\n",
      "Epoch 00032 | Loss 0.7493 | Test Acc 0.7290 | Time(s) 0.0141\n",
      "Epoch 00033 | Loss 0.7398 | Test Acc 0.7300 | Time(s) 0.0142\n",
      "Epoch 00034 | Loss 0.7304 | Test Acc 0.7300 | Time(s) 0.0142\n",
      "Epoch 00035 | Loss 0.7211 | Test Acc 0.7290 | Time(s) 0.0142\n",
      "Epoch 00036 | Loss 0.7120 | Test Acc 0.7320 | Time(s) 0.0143\n",
      "Epoch 00037 | Loss 0.7029 | Test Acc 0.7330 | Time(s) 0.0143\n",
      "Epoch 00038 | Loss 0.6940 | Test Acc 0.7330 | Time(s) 0.0142\n",
      "Epoch 00039 | Loss 0.6852 | Test Acc 0.7340 | Time(s) 0.0144\n",
      "Epoch 00040 | Loss 0.6765 | Test Acc 0.7360 | Time(s) 0.0144\n",
      "Epoch 00041 | Loss 0.6679 | Test Acc 0.7350 | Time(s) 0.0144\n",
      "Epoch 00042 | Loss 0.6594 | Test Acc 0.7340 | Time(s) 0.0145\n",
      "Epoch 00043 | Loss 0.6510 | Test Acc 0.7340 | Time(s) 0.0145\n",
      "Epoch 00044 | Loss 0.6427 | Test Acc 0.7350 | Time(s) 0.0144\n",
      "Epoch 00045 | Loss 0.6346 | Test Acc 0.7350 | Time(s) 0.0144\n",
      "Epoch 00046 | Loss 0.6265 | Test Acc 0.7350 | Time(s) 0.0144\n",
      "Epoch 00047 | Loss 0.6186 | Test Acc 0.7360 | Time(s) 0.0143\n",
      "Epoch 00048 | Loss 0.6107 | Test Acc 0.7370 | Time(s) 0.0143\n",
      "Epoch 00049 | Loss 0.6030 | Test Acc 0.7370 | Time(s) 0.0144\n",
      "Epoch 00050 | Loss 0.5954 | Test Acc 0.7370 | Time(s) 0.0144\n",
      "Epoch 00051 | Loss 0.5878 | Test Acc 0.7360 | Time(s) 0.0143\n",
      "Epoch 00052 | Loss 0.5804 | Test Acc 0.7370 | Time(s) 0.0144\n",
      "Epoch 00053 | Loss 0.5730 | Test Acc 0.7370 | Time(s) 0.0144\n",
      "Epoch 00054 | Loss 0.5658 | Test Acc 0.7380 | Time(s) 0.0143\n",
      "Epoch 00055 | Loss 0.5587 | Test Acc 0.7380 | Time(s) 0.0143\n",
      "Epoch 00056 | Loss 0.5516 | Test Acc 0.7370 | Time(s) 0.0143\n",
      "Epoch 00057 | Loss 0.5446 | Test Acc 0.7380 | Time(s) 0.0143\n",
      "Epoch 00058 | Loss 0.5378 | Test Acc 0.7380 | Time(s) 0.0142\n",
      "Epoch 00059 | Loss 0.5310 | Test Acc 0.7380 | Time(s) 0.0143\n",
      "Epoch 00060 | Loss 0.5243 | Test Acc 0.7360 | Time(s) 0.0143\n",
      "Epoch 00061 | Loss 0.5177 | Test Acc 0.7370 | Time(s) 0.0143\n",
      "Epoch 00062 | Loss 0.5112 | Test Acc 0.7380 | Time(s) 0.0142\n",
      "Epoch 00063 | Loss 0.5048 | Test Acc 0.7370 | Time(s) 0.0142\n",
      "Epoch 00064 | Loss 0.4985 | Test Acc 0.7380 | Time(s) 0.0142\n",
      "Epoch 00065 | Loss 0.4923 | Test Acc 0.7370 | Time(s) 0.0142\n",
      "Epoch 00066 | Loss 0.4861 | Test Acc 0.7360 | Time(s) 0.0142\n",
      "Epoch 00067 | Loss 0.4800 | Test Acc 0.7360 | Time(s) 0.0142\n",
      "Epoch 00068 | Loss 0.4740 | Test Acc 0.7360 | Time(s) 0.0142\n",
      "Epoch 00069 | Loss 0.4681 | Test Acc 0.7360 | Time(s) 0.0142\n",
      "Epoch 00070 | Loss 0.4623 | Test Acc 0.7360 | Time(s) 0.0141\n",
      "Epoch 00071 | Loss 0.4565 | Test Acc 0.7360 | Time(s) 0.0141\n",
      "Epoch 00072 | Loss 0.4509 | Test Acc 0.7360 | Time(s) 0.0141\n",
      "Epoch 00073 | Loss 0.4453 | Test Acc 0.7360 | Time(s) 0.0141\n",
      "Epoch 00074 | Loss 0.4398 | Test Acc 0.7350 | Time(s) 0.0141\n",
      "Epoch 00075 | Loss 0.4343 | Test Acc 0.7350 | Time(s) 0.0141\n",
      "Epoch 00076 | Loss 0.4290 | Test Acc 0.7340 | Time(s) 0.0141\n",
      "Epoch 00077 | Loss 0.4237 | Test Acc 0.7340 | Time(s) 0.0141\n",
      "Epoch 00078 | Loss 0.4185 | Test Acc 0.7330 | Time(s) 0.0141\n",
      "Epoch 00079 | Loss 0.4133 | Test Acc 0.7330 | Time(s) 0.0142\n",
      "Epoch 00080 | Loss 0.4083 | Test Acc 0.7300 | Time(s) 0.0142\n",
      "Epoch 00081 | Loss 0.4033 | Test Acc 0.7300 | Time(s) 0.0142\n",
      "Epoch 00082 | Loss 0.3983 | Test Acc 0.7300 | Time(s) 0.0142\n",
      "Epoch 00083 | Loss 0.3935 | Test Acc 0.7270 | Time(s) 0.0142\n",
      "Epoch 00084 | Loss 0.3887 | Test Acc 0.7280 | Time(s) 0.0142\n",
      "Epoch 00085 | Loss 0.3840 | Test Acc 0.7280 | Time(s) 0.0141\n",
      "Epoch 00086 | Loss 0.3793 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00087 | Loss 0.3747 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00088 | Loss 0.3702 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00089 | Loss 0.3657 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00090 | Loss 0.3613 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00091 | Loss 0.3570 | Test Acc 0.7270 | Time(s) 0.0141\n",
      "Epoch 00092 | Loss 0.3527 | Test Acc 0.7270 | Time(s) 0.0142\n",
      "Epoch 00093 | Loss 0.3485 | Test Acc 0.7270 | Time(s) 0.0142\n",
      "Epoch 00094 | Loss 0.3443 | Test Acc 0.7260 | Time(s) 0.0141\n",
      "Epoch 00095 | Loss 0.3402 | Test Acc 0.7260 | Time(s) 0.0142\n",
      "Epoch 00096 | Loss 0.3361 | Test Acc 0.7250 | Time(s) 0.0142\n",
      "Epoch 00097 | Loss 0.3321 | Test Acc 0.7250 | Time(s) 0.0142\n",
      "Epoch 00098 | Loss 0.3282 | Test Acc 0.7270 | Time(s) 0.0142\n",
      "Epoch 00099 | Loss 0.3243 | Test Acc 0.7270 | Time(s) 0.0142\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "dur = []\n",
    "for epoch in range(100):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "piano-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = net(g, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "concerned-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits[test_mask]\n",
    "labels = labels[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "attempted-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = th.max(logits, dim=1)\n",
    "correct = th.sum(indices == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "floppy-silly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 5, 5, 5, 5, 1, 1, 1, 1, 1, 1, 6,\n",
       "        1, 1, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
       "        5, 5, 1, 3, 5, 2, 3, 6, 6, 5, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 6, 0, 6,\n",
       "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 4, 4, 4, 4, 4, 3, 5, 5, 5, 5, 5,\n",
       "        6, 5, 5, 5, 5, 6, 4, 4, 0, 0, 1, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0,\n",
       "        0, 0, 0, 0, 3, 4, 0, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        6, 0, 0, 6, 0, 5, 5, 5, 5, 5, 5, 4, 4, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        3, 2, 5, 2, 2, 4, 4, 4, 3, 2, 2, 2, 3, 2, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5,\n",
       "        4, 0, 5, 6, 5, 5, 1, 0, 5, 5, 6, 5, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 1, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 5, 3, 3, 3, 3, 3,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 6, 0, 0, 0, 5, 0, 1, 1, 0, 6, 6, 6, 6, 1, 3, 3,\n",
       "        0, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 6, 0, 6, 6, 0, 0, 3, 3,\n",
       "        3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 5, 6, 0, 0, 0, 0, 5, 6, 6, 6, 6, 6, 3, 3,\n",
       "        6, 6, 6, 2, 1, 1, 0, 0, 0, 5, 5, 3, 3, 3, 0, 0, 0, 0, 0, 0, 5, 5, 0, 4,\n",
       "        6, 0, 5, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 1, 6, 1, 1,\n",
       "        3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 5, 3, 0, 0, 0, 0, 5,\n",
       "        4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 1, 1, 4, 1, 5, 5, 3, 3, 4,\n",
       "        4, 0, 4, 0, 0, 4, 0, 0, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 6, 2, 0, 5, 6, 3,\n",
       "        5, 5, 5, 5, 5, 4, 5, 4, 0, 0, 0, 0, 3, 5, 0, 5, 1, 4, 4, 3, 3, 3, 3, 2,\n",
       "        3, 4, 3, 0, 0, 5, 1, 3, 6, 3, 1, 0, 3, 0, 1, 5, 3, 1, 5, 1, 1, 1, 1, 1,\n",
       "        4, 0, 2, 4, 4, 4, 3, 1, 1, 3, 0, 3, 0, 4, 4, 0, 4, 4, 4, 3, 3, 3, 3, 0,\n",
       "        0, 0, 2, 3, 3, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 3, 0, 5, 5, 4, 1, 4, 4,\n",
       "        4, 4, 1, 4, 0, 0, 5, 4, 6, 2, 2, 2, 2, 4, 5, 6, 6, 0, 3, 4, 4, 4, 3, 3,\n",
       "        0, 5, 3, 5, 0, 0, 6, 6, 3, 2, 5, 2, 0, 0, 0, 0, 3, 2, 6, 0, 0, 0, 0, 5,\n",
       "        5, 1, 3, 0, 0, 5, 4, 4, 6, 0, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1,\n",
       "        2, 0, 6, 6, 5, 6, 6, 3, 2, 6, 5, 4, 4, 4, 2, 5, 5, 0, 3, 3, 0, 4, 4, 3,\n",
       "        2, 3, 1, 6, 6, 5, 0, 4, 4, 6, 3, 1, 1, 4, 0, 5, 2, 3, 3, 3, 0, 5, 5, 0,\n",
       "        3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 5, 0, 5, 3, 2, 2, 5, 5, 4, 3, 4, 3, 1, 1,\n",
       "        4, 2, 4, 5, 5, 5, 2, 3, 2, 3, 3, 3, 5, 5, 4, 3, 3, 3, 1, 3, 0, 0, 2, 5,\n",
       "        5, 5, 3, 3, 3, 5, 2, 3, 1, 1, 1, 3, 3, 3, 0, 4, 4, 3, 3, 3, 3, 0, 3, 3,\n",
       "        3, 3, 5, 0, 0, 3, 3, 3, 3, 2, 3, 0, 1, 2, 6, 4, 3, 2, 5, 1, 5, 0, 3, 3,\n",
       "        2, 2, 1, 3, 0, 2, 5, 5, 6, 3, 0, 2, 6, 1, 3, 0, 3, 3, 2, 4, 2, 5, 0, 0,\n",
       "        0, 5, 6, 4, 3, 3, 3, 2, 5, 3, 5, 4, 3, 3, 3, 3, 3, 4, 6, 6, 5, 2, 2, 2,\n",
       "        5, 4, 4, 4, 4, 6, 3, 2, 2, 6, 0, 0, 1, 2, 2, 3, 5, 4, 4, 3, 3, 0, 0, 3,\n",
       "        3, 0, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 0, 4, 2, 3, 3, 3, 3, 3, 2,\n",
       "        5, 3, 4, 4, 4, 3, 3, 5, 5, 3, 3, 3, 3, 5, 3, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-hacker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:size_env_1.0]",
   "language": "python",
   "name": "conda-env-size_env_1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
